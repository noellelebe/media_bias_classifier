# -*- coding: utf-8 -*-
"""Kopie von hackingCommScience_MediaBias.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ulYM2Cn8cuR9vqQaQnq5_F5kmUSmlC4Z

# Newspaper version of embeddings

Code from https://thinkinfi.com/gensim-doc2vec-python-implementation/
"""

import gensim
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import nltk
# nltk.download('punkt')
from nltk.tokenize import word_tokenize

"""Generates a balanced sample of 60,000 articles from 6 newspapers."""

import pandas as pd
import numpy as np
print("import done")
pprs = pd.read_csv("/Users/janabernhard/Documents/PhD/Unterricht/SS22_HackatonICA/full_sample.csv", encoding = "UTF-8", encoding_errors="replace")
print(pprs.columns)

pprs = pprs[["date", "text", "paper"]]

# Tokenization of each document
pprs = pprs.dropna()
pprs = pprs.reset_index()

doc = pprs.text

tokenized_doc = []
for d in doc:
    tokenized_doc.append(word_tokenize(d.lower()))
print("tokenizer done")
# Convert tokenized document into gensim formated tagged data
tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]

## Train doc2vec model
print("start training")
model = Doc2Vec(tagged_data, vector_size=300, window=5, min_count=10, workers=4, epochs = 100)
# Save trained doc2vec model
model.save("/Users/janabernhard/Documents/PhD/Unterricht/SS22_HackatonICA/media_bias_classifier/bias/nps_doc2vec.model")